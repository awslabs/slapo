{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: slapo\n\n# Optimize MLP Module on Multi-Device\n\nThis guide uses the multi-layer perceptron (MLP) module, one of the \nbasin components in Transformer-based models, as an example to show\nhow we can leverage Slapo to optimize its performance on multiple devices.\nWe will cover tensor parallelism, synchronization, and operator fusion\nin this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first import the necessary packages. Make sure you have already installed\nthe PyTorch framework.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport slapo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we will use multiple GPUs to run the model, we need to initialize the distributed\nbackend. We only initialize the CPU backend in this tutorial, but you can\ninitialize the NCCL backend on GPU by passing in ``backend=\"nccl\"``, and change\nthe actual number of devices accordingly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "slapo.env.setup(rank=0, world_size=1, backend=\"gloo\")\nprint(f\"rank: {dist.get_rank()}, world_size: {dist.get_world_size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition\n\nWe first define a MLP module that consists of two linear layers and a GELU activation,\nwhich is a basic component in Transformer-based models like GPT. Users can instantiate\nthe module as usual.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.activation = nn.GELU()\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, data):\n        out = self.linear1(data)\n        out = self.activation(out)\n        out = self.linear2(out)\n        return out\n\n\nmodel = MLP(1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Model Schedule\n\nWe then create a default schedule ``sch`` for the model. Users can always check the\ncorresponding PyTorch model by calling ``sch.mod``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch = slapo.create_schedule(model)\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensor Parallelism\n\nHere comes the most important part of transforming the single-device model to\na parallelized one. Slapo provides a ``.shard()`` primitive to realize tensor\nparallelism. Users can specify the name of the tensor and the axis to shard the\ntensor along. We follow the convention of [Megatron-LM](https://arxiv.org/abs/1909.08053)\nto shard the weight $A$ in the first linear layer by column, and the\nweight $B$ in the second linear layer by row. Consider a machine with two\ndevices, the computation becomes as follows:\n\n\\begin{align}f(XA)B = f\\left(X\\begin{bmatrix}A_1 & A_2\\end{bmatrix}\\right) \\begin{bmatrix}B_1 \\\\ B_2\\end{bmatrix} =f(XA_1)B_1 + f(XA_2)B_2\\end{align}\n\nwhere $X$ is the input tensor. Since PyTorch's ``nn.Linear`` module by default\ntransposes the weight matrix, ``axis=0`` means sharding the output dimension.\nAs each device only holds a part of the result, we need to synchronize the results\nat the end of both forward and backward pass. We can also use ``.sync()`` to specify the\nsynchronization point and strategy. Here we use ``all_reduce`` to synchronize the results\nafter the second linear layer during forward pass, and insert another ``all_reduce``\nbefore the first linear layer during backward pass. Users only need to write the following\nseveral lines of code to realize complex tensor parallelism but have no need to care about\nthe low-level implementation details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch[\"linear1\"].shard(\"weight\", axis=0)\nsch[\"linear1\"].shard(\"bias\", axis=0)\nsch[\"linear2\"].shard(\"weight\", axis=1)\nsch[\"linear2\"].sync(mode=\"fwd_post\", sync_op_or_fn=\"all_reduce\")\nsch[\"linear1\"].sync(mode=\"bwd_post\", sync_op_or_fn=\"all_reduce\")\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If lanuch this script with two devices, you can see that the weight and bias\nof the linear layers are correctly sharded, where the output dimension of\nthe first linear layer becomes half of the original one, and each device\nonly holds half of the weight.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To further verify the end-to-end numerical correctness, Slapo also provides\na ``.Verify()`` context that can be used to execute the forward function and\ncompare the results with the original module. For example, users can leverage this context\n``with slapo.Verify(sch, example_inputs=[torch.randn(2, 512, 1024)])``\nto encapsulate those ``.shard()`` and ``.sync()`` primitives.\nIf no errors are reported, the numerical correctness is guaranteed.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operator Fusion\n\nAnother optimization we can do is to fuse the GELU activation with the first\nlinear layer. We can use ``.decompose()`` to decompose the linear layer into\na matrix multiplication and a bias addition. As shown in the output below,\nthe ``nn.Linear`` layer is replaced with the predefined ``LinearWithSeparateBias``\nmodule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch[\"linear1\"].decompose()\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To enable operator fusion, we need a static dataflow graph. Here, we explicitly\ncall ``.trace()`` to trace the module and break the linear layer into two separate\nmultiply and add operators. Users can easily determine whether they want their\ndataflow graph to be flattened or not by just passing in a flag.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.trace(flatten=True)\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Later, we define a pattern for matching the bias addition and GELU activation.\nNotice Slapo supports different types of patterns, including subgraphs with multiple\ninputs and fuzzy matching, which provides users enough flexibility to express\ntheir subgraphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pattern(x, bias):\n    x = F.gelu(bias + x)\n    return x\n\n\nsubgraph = sch.find(pattern)\nprint(subgraph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the subgraph consists of two nodes, one for the bias addition and\nthe other for the GELU activation. We can then fuse the subgraph into a single\nnode by calling ``.fuse()``. By default, Slapo will use TorchScript with nvFuser\nas the backend compiler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.fuse(subgraph, compiler=\"TorchScript\", name=\"BiasGeLU\")\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the Optimized Model\n\nWe can see the previous sharding optimization is still preserved, and the fused\nkernel is correctly inserted into the hierarchical module definition and the\ncorresponding dataflow graph.\n\nFinally, we can build the optimized model by calling ``.build()``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_model, _ = slapo.build(sch, init_weights=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}