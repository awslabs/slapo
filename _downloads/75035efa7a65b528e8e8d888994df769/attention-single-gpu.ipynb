{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: slapo\n\n# Optimize Attention Module on A Single Device\n\nThis guide uses the [Attention](https://arxiv.org/abs/1706.03762) module,\nthe core and most time-consuming module in Transformer-based models, as an\nexample to show how we can leverage Slapo to optimize its performance on\na single device. We will cover module tracing, pattern matching, operator\nfusion, and partial module replacement in this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first import the necessary packages. Make sure you have already installed\nthe PyTorch framework.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport slapo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition\n\nThe Attention module consists of SelfAttention and Projection modules, where\nSelfAttention takes in the hidden states and passes it through three different\nlinear layers to generate the query, key and value tensors. Then, those tensors\nwill be performed the following scaled dot-product attention:\n\n\\begin{align}\\mathrm{CoreAttention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^{\\mathrm{T}}}{\\sqrt{d_k}}\\right) \\cdot V\\end{align}\n\nwhere $d_k$ is the hidden dimension. Finally, the output of the attention\nmodule will be passed through a linear projection layer, added with the residual\nconnection, and conducted a layer norm to generate the final output.\nThe following code shows the implementation of the Attention module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v):\n    # (bs, head, seq, hs // head)\n    d_k = q.shape[-1]\n    attn_score = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n    # (bs, head, seq, seq)\n    attn_probs = F.softmax(attn_score, dim=-1)\n    attn_probs = F.dropout(attn_probs, 0.1)\n    # (bs, head, seq, hs // head)\n    attn = torch.matmul(attn_probs, v)\n    return attn\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, hidden_size, n_heads):\n        super().__init__()\n        self.q_proj = nn.Linear(hidden_size, hidden_size)\n        self.k_proj = nn.Linear(hidden_size, hidden_size)\n        self.v_proj = nn.Linear(hidden_size, hidden_size)\n        self.n_heads = n_heads\n\n    def permute_for_scores(self, x):\n        # x: (batch_size, seq_len, hidden_size)\n        new_shape = x.shape[:-1] + (self.n_heads, -1)\n        x = x.view(new_shape)\n        # output: (bs, head, seq, hs // head)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        # hidden_states: (batch_size, seq_len, hidden_size)\n        # qkv layers\n        q = self.permute_for_scores(self.q_proj(hidden_states))\n        k = self.permute_for_scores(self.k_proj(hidden_states))\n        v = self.permute_for_scores(self.v_proj(hidden_states))\n        # core attention\n        output = scaled_dot_product(q, k, v)\n        # output: (bs, seq, head, hs // head)\n        output.permute(0, 2, 1, 3)\n        output.view(output.shape[0], output.shape[1], -1)\n        return output\n\n\nclass Projection(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.dense = nn.Linear(hidden_size, hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.layer_norm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size, n_heads):\n        super().__init__()\n        self.self_attn = SelfAttention(hidden_size, n_heads)\n        self.proj = Projection(hidden_size)\n\n    def forward(self, hidden_states):\n        self_output = self.self_attn(hidden_states)\n        attention_output = self.proj(self_output, hidden_states)\n        return attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Users can instantiate the model based on the above definition as usual.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Attention(hidden_size=1024, n_heads=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Model Schedule\n\nLater, we pass the model to Slapo and create a default schedule for it.\nThe schedule always includes the original or the transformed module.\nUsers can check the module by calling the ``mod`` attribute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch = slapo.create_schedule(model)\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, Slapo works seamlessly with the PyTorch models and preserves\nthe hierarchical structure of the original model. As we have not added any\noptimizations, the module is exactly the same as the original one.\nWe can easily obtain the submodules by passing the module name to the schedule,\nwhich will return a new schedule for the submodule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "attn_sch = sch[\"self_attn\"]\nprint(attn_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is also the idea of progressive optimization -- we only apply optimizations\nto a small part of the model at a time and do not affect other parts.\nIf no optimizations are applied, then no changes will be made to the model, which\nis different from the traditional static graph optimization employed by deep\nlearning compilers.\n\nIn the following, we will show how to gradually apply optimizations to the model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize SelfAttention Module\n\n### Replace QKV Linear Layers\n\nSince the three linear layers in the SelfAttention module are independent, we\ncan merge them into a single linear layer to reduce the number of GEMM\noperations, and thus reduce the kernel launch overheads.\n\nThe first thing to do is to find those three linear layers and the consequential\noperations in the model. Slapo provides an easy-to-use API to help users\ndefine the pattern and find the corresponding module or subgraph in the model.\nWe can define a subgraph pattern function as shown below. The ``call_module``\nfunction will try to match a call node that satisfies the user-defined\nconstraint in the dataflow graph. The first argument specifies the name of the\nmodule, where regular expression is supported, so it can support fuzzy\nmatching in this case. The latter arguments are the arguments of the call node.\nUsing this function, we can use just one line of code to match the three linear\nlayers. Also, we need to incorporate the ``view`` and ``permute`` operations,\nwhich should also be fused together instead of doing three times separately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from slapo.pattern import call_module\n\n\ndef pattern(x):\n    x = call_module(r\"[qkv]_proj\", x)\n    new_shape = x.shape[:-1] + (16, -1)\n    x = x.view(new_shape)\n    return x.permute(0, 2, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the pattern, we can use the ``.find()`` primitive to find the\ncorresponding subgraph in the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qkv_subgraphs = attn_sch.find(pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The primitive basically does two things. First, it will `implicitly` trace the submodule\ninto a static subgraph. Currently, we use [torch.fx](https://pytorch.org/docs/stable/fx.html)\nas the IR, so the traced module will become a ``torch.fx.GraphModule``, and we can also\nsee the forward function of it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(attn_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second, the ``.find()`` primitive will return a list of subgraphs that\nmatch the pattern. In our case, there will be three subgraphs, one for each\nlinear layer and the consequential ``view`` and ``permute`` operations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(qkv_subgraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define a fused QKV module as follows and instantiate it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class FusedQKV(nn.Module):\n    def __init__(self, hidden_size, n_heads) -> None:\n        super().__init__()\n        self.n_heads = n_heads\n        self.fused_linear = nn.Linear(hidden_size, hidden_size * 3)\n\n    def permute_for_scores(self, x):\n        new_shape = x.shape[:-1] + (self.n_heads, -1)\n        x = x.view(new_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        qkv = self.fused_linear(hidden_states)\n        reshaped_qkv = self.permute_for_scores(qkv)\n        q, k, v = torch.split(reshaped_qkv, 1, dim=-1)\n        q = torch.squeeze(q, -1).contiguous()\n        k = torch.squeeze(k, -1).contiguous()\n        v = torch.squeeze(v, -1).contiguous()\n        return [q, k, v]\n\n\nfused_qkv = FusedQKV(hidden_size=1024, n_heads=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can replace the subgraphs with the fused QKV module by calling the\n``.replace()`` primitive. The first argument is the new module,\nand the second argument is the subgraph to be replaced.\nAfter replacing the subgraph, we can check the model again to see the\nchanges.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "attn_sch.replace(fused_qkv, qkv_subgraphs)\nprint(attn_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above output, we can see there is a new module called ``FusedQKV_0``\nwith $3\\times$ ``out_features`` compared to the original linear layer.\nThe corresponding forward function is also changed to leverage the fused\nmodule.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replace Scaled Dot-Product Attention\n\nNext, we still use the ``.find()`` primitive to find the core attention function\nand replace it with a more efficient implementation. Different from the QKV example\nthat requires us to explicitly write the fuzzy pattern, we can directly write\na function with the identical computation subgraph as the pattern. Since the\n``scaled_dot_product`` function has been defined previously, we can reuse it\nand pass it into ``.find()``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "core_attn_subgraph = attn_sch.find(scaled_dot_product)\nprint(core_attn_subgraph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the ``FlashAttentionOp`` provided by Slapo that makes use\nof [flash attention](https://arxiv.org/abs/2205.14135) kernels from\n[xFormers](https://github.com/facebookresearch/xformers) and\n[flash-attention](https://github.com/HazyResearch/flash-attention) libraries\nto replace the core attention. We directly import and replace the subgraph\nwith ``FlashAttentionOp``.\nNotice, since the ``scaled_dot_product`` function we defined above only accepts\nthe ``query``, ``key``, and ``value`` tensors, while ``FlashAttentionOp`` requires\nfive arguments, so we need to explicitly pass ``None`` to the ``attention_mask``\nargument, and set the dropout probability ``p`` to 0.1 by setting the ``concrete_args``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  We use ``native_xformers`` in this tutorial to demonstrate the functionality.\n  In reality, users can choose ``cutlass``, ``triton``, or ``cuda`` kernels to achieve\n  better performance, while the latter two only support NVIDIA V100 GPU.\n  Please refer to `slapo.op.attention.FlashAttentionOp` for more details.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from slapo.op.attention import FlashAttentionOp\n\nflash_attn = FlashAttentionOp(attn_op_name=\"native_xformers\", apply_causal_mask=False)\nattn_sch.replace(\n    flash_attn, core_attn_subgraph, concrete_args={\"attention_mask\": None, \"p\": 0.1}\n)\nprint(attn_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, the ``FlashAttentionOp`` is attached to the GraphModule, and the forward\nfunction becomes much simpler to call those two submodules.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize the Projection Module\n\nWe then optimize the ``Projection`` module. A common practice is to fuse the\ndropout and the layer norm layer with those element-wise addition operations.\nWe first create a subschedule for the ``Projection`` module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj_sch = sch[\"proj\"]\nprint(proj_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we want to fuse the linear bias with the consequential layers, we need to\ndecompose the linear layer into two separate matrix multiplication and bias add operations.\nIn Slapo, this is easy to achieve by simply calling ``.decompose()`` on the linear module.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  The default [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) module\n  in PyTorch will directly pass both weight and bias to the backend\n  [F.linear](https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html) function,\n  and dispatch it to the corresponding C/CUDA library, so there is no way to fuse the bias if we\n  do not take it apart. Another reason for decomposing is that we can still optimize the\n  ``weight`` parameter later (e.g., sharding) even though the ``bias`` may be fused.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj_sch[\"dense\"].decompose()\nprint(proj_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see the ``Linear`` module changed into ``LinearWithSeparateBias``, and other submodules\nremain the same. Next, we need to `explicitly` call the ``.trace()`` primitive to trace the module\ninto a static subgraph. It gives us more control over the traced module. For example, we can\npass in the ``flatten`` flag to let the tracer gets into each submodule so that the bias add\ncan be depicted as a node in the subgraph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj_sch.trace(flatten=True)\nprint(proj_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can again define the fusion pattern as follows. Here the pattern includes three input arguments,\nSlapo can still handle it correctly and grab all the required nodes in the subgraph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ln_pattern(x, bias, residual):\n    return F.layer_norm(F.dropout(x + bias) + residual, 1024)\n\n\nln_subgraph = proj_sch.find(ln_pattern)\nprint(ln_subgraph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this case of vertical fusion, Slapo provides a ``.fuse()`` primitive to easily fuse the subgraph.\nUsers can specify the backend fusion compiler and the name of the fused module. By default, Slapo\nwill use TorchScript with nvFuser to fuse the subgraph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj_sch.fuse(ln_subgraph, compiler=\"TorchScript\", name=\"FusedLayerNorm\")\nprint(proj_sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown in the above output, the ``FusedLayerNorm`` module is attached to the GraphModule, and\nonly ``torch._C._nn.Linear`` and ``FusedLayerNorm`` are called in the forward function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the Optimized Model\n\nFinally, we finish all the optimizations for Attention module on a single device.\nWe can pass the schedule into ``sch.build`` to build the optimized model for execution.\nIt returns the optimized model and a default optimizer. We can print out the top-level module\nto see the changes. The optimizations are clearly reflected in the new module, and we still\nkeep the module hierarchy, which greatly enhances the readability and debuggability of the code.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_model, _ = slapo.build(sch, init_weights=False)\nprint(opt_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}