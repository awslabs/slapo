
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/quick-start.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_quick-start.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_quick-start.py:


.. currentmodule:: slapo

Quick Start
===========

This guide walks through the key functionality of Slapo.
We will use the BERT model in `HuggingFace Hub <https://github.com/huggingface/transformers>`_ as an example and leverage Slapo to optimize its performance.

.. GENERATED FROM PYTHON SOURCE LINES 14-17

Optimize PyTorch model with Slapo
---------------------------------
We first import the Slapo package. Make sure you have already installed the PyTorch package.

.. GENERATED FROM PYTHON SOURCE LINES 17-21

.. code-block:: default


    import slapo
    import torch








.. GENERATED FROM PYTHON SOURCE LINES 22-23

We load a BERT model implemented in PyTorch from HuggingFace Hub.

.. GENERATED FROM PYTHON SOURCE LINES 23-30

.. code-block:: default


    from transformers import BertLMHeadModel, AutoConfig

    config = AutoConfig.from_pretrained("bert-large-uncased")
    model = BertLMHeadModel(config)
    print(model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]    Downloading (…)lve/main/config.json: 100%|##########| 571/571 [00:00<00:00, 147kB/s]
    BertLMHeadModel(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 1024, padding_idx=0)
          (position_embeddings): Embedding(512, 1024)
          (token_type_embeddings): Embedding(2, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0-23): 24 x BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (cls): BertOnlyMLMHead(
        (predictions): BertLMPredictionHead(
          (transform): BertPredictionHeadTransform(
            (dense): Linear(in_features=1024, out_features=1024, bias=True)
            (transform_act_fn): GELUActivation()
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          )
          (decoder): Linear(in_features=1024, out_features=30522, bias=True)
        )
      )
    )




.. GENERATED FROM PYTHON SOURCE LINES 31-42

After we have the model defintion, we can create a schedule and optimize it.
Slapo provides an ``apply_schedule`` API for users to directly apply a predefined
schedule to the model. By default, the schedule will inject the
`Flash Attention <https://arxiv.org/abs/2205.14135>`_ kernel, conduct tensor
parallelism, and fuse the operators. Users can also customize the schedule by
passing in the schedule configurations like data type (fp16/bf16) or checkpoint ratio.
Detailed schedule configurations can be found in ``slapo.model_schedule``.

After applying the schedule, we can build the optimized model by calling ``slapo.build``.
Here we explicitly pass in the ``_init_weights`` function of HuggingFace models to
initialize the parameters of the optimized model.

.. GENERATED FROM PYTHON SOURCE LINES 42-54

.. code-block:: default



    def apply_and_build_schedule(model, config):
        from slapo.model_schedule import apply_schedule

        sch = apply_schedule(
            model, "bert", model_config=config, prefix="bert", fp16=True, ckpt_ratio=0
        )
        opt_model, _ = slapo.build(sch, init_weights=model._init_weights)
        return opt_model









.. GENERATED FROM PYTHON SOURCE LINES 55-57

The optimized model is still a PyTorch ``nn.Module``, so we can pass it to the
PyTorch training loop as usual.

.. GENERATED FROM PYTHON SOURCE LINES 57-74

.. code-block:: default



    def train(model, device="cuda", bs=8, seq_length=512):
        input_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)
        attention_mask = torch.ones(bs, seq_length, dtype=torch.float16, device=device)
        token_type_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)
        labels = input_ids.clone()
        model.to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
        for step in range(100):
            inputs = (input_ids, attention_mask, token_type_ids)
            loss = model(*inputs, labels=labels).loss
            loss.backward()
            optimizer.step()

            if step % 10 == 0:
                print(f"step {step} loss: {loss.item()}")








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  5.038 seconds)


.. _sphx_glr_download_gallery_quick-start.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: quick-start.py <quick-start.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: quick-start.ipynb <quick-start.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
