

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Optimize MLP Module on Multi-Device &#8212; Slapo Documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gallery/mlp-multi-gpu';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Debugging with Print" href="debug-print.html" />
    <link rel="prev" title="Optimize Attention Module on A Single Device" href="attention-single-gpu.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">Slapo Documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../setup/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick-start.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="attention-single-gpu.html">Optimize Attention Module on A Single Device</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Optimize MLP Module on Multi-Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug-print.html">Debugging with Print</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../python_api/index.html">Python API</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_api/root.html">slapo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/schedule.html">slapo.schedule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/initialization.html">slapo.initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/pattern.html">slapo.pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/pipeline.html">slapo.pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/tracer.html">slapo.tracer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/random.html">slapo.random</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../python_api/framework_dialect/index.html">slapo.framework_dialect</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../python_api/framework_dialect/registry.html">slapo.framework_dialect.registry</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../python_api/framework_dialect/deepspeed/index.html">slapo.framework_dialect.deepspeed</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../python_api/framework_dialect/deepspeed/engine.html">slapo.framework_dialect.deepspeed.engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../python_api/framework_dialect/deepspeed/pipeline.html">slapo.framework_dialect.deepspeed.pipeline</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../python_api/model_schedule/index.html">slapo.model_schedule</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../python_api/model_schedule/api.html">slapo.model_schedule.api</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../python_api/op/index.html">slapo.op</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/attention.html">slapo.op.attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/cross_entropy.html">slapo.op.cross_entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/fused_bias.html">slapo.op.fused_bias</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/linear.html">slapo.op.linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/mlp.html">slapo.op.mlp</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/awslabs/slapo" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/gallery/mlp-multi-gpu.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimize MLP Module on Multi-Device</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">Model Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model-schedule">Create Model Schedule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallelism">Tensor Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-fusion">Operator Fusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-the-optimized-model">Build the Optimized Model</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-gallery-mlp-multi-gpu-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="optimize-mlp-module-on-multi-device">
<span id="sphx-glr-gallery-mlp-multi-gpu-py"></span><h1>Optimize MLP Module on Multi-Device<a class="headerlink" href="#optimize-mlp-module-on-multi-device" title="Permalink to this headline"><span>¶</span></a></h1>
<p>This guide uses the multi-layer perceptron (MLP) module, one of the
basin components in Transformer-based models, as an example to show
how we can leverage Slapo to optimize its performance on multiple devices.
We will cover tensor parallelism, synchronization, and operator fusion
in this tutorial.</p>
<p>We first import the necessary packages. Make sure you have already installed
the PyTorch framework.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">slapo</span>
</pre></div>
</div>
<p>Since we will use multiple GPUs to run the model, we need to initialize the distributed
backend. We only initialize the CPU backend in this tutorial, but you can
initialize the NCCL backend on GPU by passing in <code class="docutils literal notranslate"><span class="pre">backend=&quot;nccl&quot;</span></code>, and change
the actual number of devices accordingly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">slapo</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;gloo&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;rank: </span><span class="si">{</span><a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_rank" title="torch.distributed.get_rank" class="sphx-glr-backref-module-torch-distributed sphx-glr-backref-type-py-function"><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span></a><span class="p">()</span><span class="si">}</span><span class="s2">, world_size: </span><span class="si">{</span><a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_world_size" title="torch.distributed.get_world_size" class="sphx-glr-backref-module-torch-distributed sphx-glr-backref-type-py-function"><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span></a><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>rank: 0, world_size: 1
</pre></div>
</div>
<div class="section" id="model-definition">
<h2>Model Definition<a class="headerlink" href="#model-definition" title="Permalink to this headline"><span>¶</span></a></h2>
<p>We first define a MLP module that consists of two linear layers and a GELU activation,
which is a basic component in Transformer-based models like GPT. Users can instantiate
the module as usual.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU" title="torch.nn.GELU" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span></a><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="n">model</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">MLP</span></a><span class="p">(</span><span class="mi">1024</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="create-model-schedule">
<h2>Create Model Schedule<a class="headerlink" href="#create-model-schedule" title="Permalink to this headline"><span>¶</span></a></h2>
<p>We then create a default schedule <code class="docutils literal notranslate"><span class="pre">sch</span></code> for the model. Users can always check the
corresponding PyTorch model by calling <code class="docutils literal notranslate"><span class="pre">sch.mod</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sch</span> <span class="o">=</span> <span class="n">slapo</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>MLP(
  (linear1): Linear(in_features=1024, out_features=1024, bias=True)
  (activation): GELU(approximate=&#39;none&#39;)
  (linear2): Linear(in_features=1024, out_features=1024, bias=True)
)
</pre></div>
</div>
</div>
<div class="section" id="tensor-parallelism">
<h2>Tensor Parallelism<a class="headerlink" href="#tensor-parallelism" title="Permalink to this headline"><span>¶</span></a></h2>
<p>Here comes the most important part of transforming the single-device model to
a parallelized one. Slapo provides a <code class="docutils literal notranslate"><span class="pre">.shard()</span></code> primitive to realize tensor
parallelism. Users can specify the name of the tensor and the axis to shard the
tensor along. We follow the convention of <a class="reference external" href="https://arxiv.org/abs/1909.08053">Megatron-LM</a>
to shard the weight <span class="math notranslate nohighlight">\(A\)</span> in the first linear layer by column, and the
weight <span class="math notranslate nohighlight">\(B\)</span> in the second linear layer by row. Consider a machine with two
devices, the computation becomes as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(XA)B = f\left(X\begin{bmatrix}A_1 &amp; A_2\end{bmatrix}\right) \begin{bmatrix}B_1 \\ B_2\end{bmatrix} =f(XA_1)B_1 + f(XA_2)B_2\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(X\)</span> is the input tensor. Since PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> module by default
transposes the weight matrix, <code class="docutils literal notranslate"><span class="pre">axis=0</span></code> means sharding the output dimension.
As each device only holds a part of the result, we need to synchronize the results
at the end of both forward and backward pass. We can also use <code class="docutils literal notranslate"><span class="pre">.sync()</span></code> to specify the
synchronization point and strategy. Here we use <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code> to synchronize the results
after the second linear layer during forward pass, and insert another <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code>
before the first linear layer during backward pass. Users only need to write the following
several lines of code to realize complex tensor parallelism but have no need to care about
the low-level implementation details.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sch</span><span class="p">[</span><span class="s2">&quot;linear1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sch</span><span class="p">[</span><span class="s2">&quot;linear1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sch</span><span class="p">[</span><span class="s2">&quot;linear2&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sch</span><span class="p">[</span><span class="s2">&quot;linear2&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sync</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;fwd_post&quot;</span><span class="p">,</span> <span class="n">sync_op_or_fn</span><span class="o">=</span><span class="s2">&quot;all_reduce&quot;</span><span class="p">)</span>
<span class="n">sch</span><span class="p">[</span><span class="s2">&quot;linear1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sync</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bwd_post&quot;</span><span class="p">,</span> <span class="n">sync_op_or_fn</span><span class="o">=</span><span class="s2">&quot;all_reduce&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>MLP(
  (linear1): Linear(in_features=1024, out_features=1024, bias=True)
  (activation): GELU(approximate=&#39;none&#39;)
  (linear2): LinearWithSyncFunc(in_features=1024, out_features=1024, bias=True, sync_fn=partial(reduce_forward_output, group=None))
)
</pre></div>
</div>
<p>If lanuch this script with two devices, you can see that the weight and bias
of the linear layers are correctly sharded, where the output dimension of
the first linear layer becomes half of the original one, and each device
only holds half of the weight.</p>
</div>
<div class="section" id="operator-fusion">
<h2>Operator Fusion<a class="headerlink" href="#operator-fusion" title="Permalink to this headline"><span>¶</span></a></h2>
<p>Another optimization we can do is to fuse the GELU activation with the first
linear layer. We can use <code class="docutils literal notranslate"><span class="pre">.decompose()</span></code> to decompose the linear layer into
a matrix multiplication and a bias addition. As shown in the output below,
the <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> layer is replaced with the predefined <code class="docutils literal notranslate"><span class="pre">LinearWithSeparateBias</span></code>
module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sch</span><span class="p">[</span><span class="s2">&quot;linear1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">decompose</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>MLP(
  (linear1): LinearWithSeparateBias(in_features=1024, out_features=1024, bias=True)
  (activation): GELU(approximate=&#39;none&#39;)
  (linear2): LinearWithSyncFunc(in_features=1024, out_features=1024, bias=True, sync_fn=partial(reduce_forward_output, group=None))
)
</pre></div>
</div>
<p>To enable operator fusion, we need a static dataflow graph. Here, we explicitly
call <code class="docutils literal notranslate"><span class="pre">.trace()</span></code> to trace the module and break the linear layer into two separate
multiply and add operators. Users can easily determine whether they want their
dataflow graph to be flattened or not by just passing in a flag.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>GraphModule(
  (linear1): Module()
  (activation): GELU(approximate=&#39;none&#39;)
  (linear2): LinearWithSyncFunc(in_features=1024, out_features=1024, bias=True, sync_fn=partial(reduce_forward_output, group=None))
)



def forward(self, data):
    linear1_weight = self.linear1.weight
    linear = torch._C._nn.linear(data, linear1_weight, None);  data = linear1_weight = None
    linear1_bias = self.linear1.bias
    add = linear + linear1_bias;  linear = linear1_bias = None
    activation = self.activation(add);  add = None
    linear2 = self.linear2(activation);  activation = None
    return linear2

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>Later, we define a pattern for matching the bias addition and GELU activation.
Notice Slapo supports different types of patterns, including subgraphs with multiple
inputs and fuzzy matching, which provides users enough flexibility to express
their subgraphs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pattern</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu" title="torch.nn.functional.gelu" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">gelu</span></a><span class="p">(</span><span class="n">bias</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">subgraph</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subgraph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[(&#39;&#39;, add), (&#39;&#39;, activation)]]
</pre></div>
</div>
<p>As expected, the subgraph consists of two nodes, one for the bias addition and
the other for the GELU activation. We can then fuse the subgraph into a single
node by calling <code class="docutils literal notranslate"><span class="pre">.fuse()</span></code>. By default, Slapo will use TorchScript with nvFuser
as the backend compiler.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sch</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">subgraph</span><span class="p">,</span> <span class="n">compiler</span><span class="o">=</span><span class="s2">&quot;TorchScript&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;BiasGeLU&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn&#39;t support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(&quot;The TorchScript type system doesn&#39;t support &quot;
GraphModule(
  (linear1): Module()
  (linear2): LinearWithSyncFunc(in_features=1024, out_features=1024, bias=True, sync_fn=partial(reduce_forward_output, group=None))
  (BiasGeLU_0): RecursiveScriptModule(
    original_name=GraphModule
    (activation): RecursiveScriptModule(original_name=GELU)
  )
)



def forward(self, data):
    linear1_weight = self.linear1.weight
    linear = torch._C._nn.linear(data, linear1_weight, None);  data = linear1_weight = None
    linear1_bias = self.linear1.bias
    bias_ge_lu_0 = self.BiasGeLU_0(linear, linear1_bias);  linear = linear1_bias = None
    linear2 = self.linear2(bias_ge_lu_0);  bias_ge_lu_0 = None
    return linear2

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
</div>
<div class="section" id="build-the-optimized-model">
<h2>Build the Optimized Model<a class="headerlink" href="#build-the-optimized-model" title="Permalink to this headline"><span>¶</span></a></h2>
<p>We can see the previous sharding optimization is still preserved, and the fused
kernel is correctly inserted into the hierarchical module definition and the
corresponding dataflow graph.</p>
<p>Finally, we can build the optimized model by calling <code class="docutils literal notranslate"><span class="pre">.build()</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">opt_model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">slapo</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">init_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.076 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-gallery-mlp-multi-gpu-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3f543fc069f615fa6b9b1314f0b64121/mlp-multi-gpu.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">mlp-multi-gpu.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/25a0ff26264d3ea175c18f0db0e80b9c/mlp-multi-gpu.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">mlp-multi-gpu.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="attention-single-gpu.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Optimize Attention Module on A Single Device</p>
      </div>
    </a>
    <a class="right-next"
       href="debug-print.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Debugging with Print</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">Model Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model-schedule">Create Model Schedule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallelism">Tensor Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-fusion">Operator Fusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-the-optimized-model">Build the Optimized Model</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Amazon
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, Amazon.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>