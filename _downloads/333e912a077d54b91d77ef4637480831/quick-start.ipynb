{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: slapo\n\n# Quick Start\n\nThis guide walks through the key functionality of Slapo.\nWe will use the BERT model in [HuggingFace Hub](https://github.com/huggingface/transformers) as an example and leverage Slapo to optimize its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize PyTorch model with Slapo\nWe first import the Slapo package. Make sure you have already installed the PyTorch package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import slapo\nimport torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load a BERT model implemented in PyTorch from HuggingFace Hub.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from transformers import BertLMHeadModel, AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"bert-large-uncased\")\nmodel = BertLMHeadModel(config)\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we have the model defintion, we can create a schedule and optimize it.\nSlapo provides an ``apply_schedule`` API for users to directly apply a predefined\nschedule to the model. By default, the schedule will inject the\n[Flash Attention](https://arxiv.org/abs/2205.14135) kernel, conduct tensor\nparallelism, and fuse the operators. Users can also customize the schedule by\npassing in the schedule configurations like data type (fp16/bf16) or checkpoint ratio.\nDetailed schedule configurations can be found in ``slapo.model_schedule``.\n\nAfter applying the schedule, we can build the optimized model by calling ``slapo.build``.\nHere we explicitly pass in the ``_init_weights`` function of HuggingFace models to\ninitialize the parameters of the optimized model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def apply_and_build_schedule(model, config):\n    from slapo.model_schedule import apply_schedule\n\n    sch = apply_schedule(\n        model, \"bert\", model_config=config, prefix=\"bert\", fp16=True, ckpt_ratio=0\n    )\n    opt_model, _ = slapo.build(sch, init_weights=model._init_weights)\n    return opt_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optimized model is still a PyTorch ``nn.Module``, so we can pass it to the\nPyTorch training loop as usual.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(model, device=\"cuda\", bs=8, seq_length=512):\n    input_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)\n    attention_mask = torch.ones(bs, seq_length, dtype=torch.float16, device=device)\n    token_type_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)\n    labels = input_ids.clone()\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n    for step in range(100):\n        inputs = (input_ids, attention_mask, token_type_ids)\n        loss = model(*inputs, labels=labels).loss\n        loss.backward()\n        optimizer.step()\n\n        if step % 10 == 0:\n            print(f\"step {step} loss: {loss.item()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}