{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: slapo\n\n# Quick Start\n\nThis guide walks through the key functionality of Slapo.\nWe will use the BERT model in [HuggingFace Hub](https://github.com/huggingface/transformers) as an example and leverage Slapo to optimize its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize PyTorch model with Slapo\nWe first import the Slapo package. Make sure you have already installed the PyTorch package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import slapo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then load a BERT model implemented in PyTorch from HuggingFace Hub.\nThis is the model definition part.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from transformers import BertLMHeadModel, AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"bert-large-uncased\")\nbert = BertLMHeadModel(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we have the model defintion, we can create a default schedule `sch`.\nLater on, all the optimizations will be conducted on this schedule,\nand we do not need to directly modify the original model.\nThe original module is stored in the :class:`~slapo.Schedule`, and can be accessed by `sch.mod`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch = slapo.create_schedule(bert)\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above output, we can see that the original hierachical structure\nis preserved in Slapo schedule. Actually, since we have not added any\noptimizations, the model should be exactly the same as the vanilla PyTorch one.\nUsers can leverage this structure to access the inner modules to\nconduct optimizations. For example, we can use the following code to access\nthe first attention layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subsch = sch[\"bert.encoder.layer.0.attention\"]\nprint(subsch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output submodule is just a part of the original one, but this helps users\nto quickly locate the submodule they need.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}