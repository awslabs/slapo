
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/debug-print.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_debug-print.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_debug-print.py:


.. currentmodule:: slapo

Debugging with Print
====================

Although Slapo only traces a sub-module when we have to schedule its computational
graph, it is still annoying to debug the numerical correctness of traced sub-modules.
One important reason is that the traced sub-module becomes a GraphModule, which
computational graph is the traced IR graph in torch.fx. It means the forward function
of the sub-module is only evaluated when generating torch.fx IR graph instead of
the model execution. Therefore, we cannot print the intermediate values of
the sub-module during runtime.

To solve this problem, we provide a custom module ``Print`` in Slapo. This module
is marked as a leaf in our tracer, which means it will be preserved in the traced
graph and can be evaluated in runtime.

In this turorial, we will show how to use ``Print`` to print the intermediate
values of a sub-module.

.. GENERATED FROM PYTHON SOURCE LINES 26-27

We first import the Slapo package. Make sure you have already installed PyTorch.

.. GENERATED FROM PYTHON SOURCE LINES 27-32

.. code-block:: default


    import torch
    import torch.nn as nn
    import slapo








.. GENERATED FROM PYTHON SOURCE LINES 33-36

We define a MLP module that consists of two linear layers and a GELU activation
as an example in this tutorial. You can notice that we add a ``Print`` module
to print the intermediate output of the first Linear layer.

.. GENERATED FROM PYTHON SOURCE LINES 36-67

.. code-block:: default



    class MLPWithPrint(nn.Module):
        def __init__(self, hidden_size):
            super().__init__()
            self.print = slapo.op.Print()
            self.linear1 = nn.Linear(hidden_size, hidden_size)
            self.activation = nn.GELU()
            self.linear2 = nn.Linear(hidden_size, hidden_size)

        def forward(self, data):
            out = self.linear1(data)
            out = self.print(out, "linear1 shape\n", out.shape, "\nvalue\n", out)
            out = self.activation(out)
            out = self.linear2(out)
            return out


    # You may feel the usage of `self.print` looks weird. This is because `self.print`
    # has to return a tensor, and the returned tensor has to be consumed by the
    # next operator/module, making it a part of the dataflow graph; otherwise
    # `self.print` will be removed by the tracer because it is dead code.
    # From a dataflow's point of view, you can treat `out = self.print(out, ...)` as
    # a statement of identical assignment (i.e., `out = out`).

    # Starting from the second argument are the arguments of normal Python `print`.
    # However, you have to make sure the values you printed are evaluated lazily.
    # Specifically, in this example, we specify `out` in the 3rd argument instead of
    # a part of the string in 2nd argument, so that it will be evaluated in runtime.
    # We will show some incorrect usages of `self.print` in the end of this tutorial.








.. GENERATED FROM PYTHON SOURCE LINES 68-69

Now let's create a schedule and trace the module.

.. GENERATED FROM PYTHON SOURCE LINES 69-85

.. code-block:: default


    model = MLPWithPrint(4)
    sch = slapo.create_schedule(model)
    sch.trace()

    # And here is the traced torch.fx graph. We can see that `self.print` becomes
    # an operator in the graph with the output of linear1 as its arguments.
    print(sch.mod.code)

    # We then build and execute the model:
    model, _ = slapo.build(sch, init_weights=False)
    data = torch.randn((2, 2, 4))
    model(data)

    # The linear1's output is printed!





.. rst-class:: sphx-glr-script-out

 .. code-block:: none




    def forward(self, data):
        linear1 = self.linear1(data);  data = None
        getattr_1 = linear1.shape
        print_1 = self.print(linear1, 'linear1 shape\n', getattr_1, '\nvalue\n', linear1);  linear1 = getattr_1 = None
        activation = self.activation(print_1);  print_1 = None
        linear2 = self.linear2(activation);  activation = None
        return linear2
    
    linear1 shape
     torch.Size([2, 2, 4]) 
    value
     tensor([[[ 0.0602,  0.6097,  0.5330, -0.1625],
             [ 0.9134, -0.3139,  1.1942,  0.2745]],

            [[-0.5527,  0.7517,  0.2346,  0.2881],
             [-0.5587,  1.1083,  0.2211, -0.7309]]], grad_fn=<ViewBackward0>)

    tensor([[[ 0.4465,  0.2216, -0.1530,  0.0024],
             [ 0.9929,  0.3699, -0.1248,  0.4279]],

            [[ 0.2741,  0.0811, -0.2174, -0.0075],
             [ 0.3500,  0.2996, -0.2580, -0.2106]]], grad_fn=<ViewBackward0>)



.. GENERATED FROM PYTHON SOURCE LINES 86-89

On the other hand, as we have mentioned above, the print won't work properly
if the values you want to print are evaluated when tracing. Here is an example
that shows incorrect usages of `self.print`.

.. GENERATED FROM PYTHON SOURCE LINES 89-109

.. code-block:: default



    class MLPWithWrongPrint(nn.Module):
        def __init__(self, hidden_size):
            super().__init__()
            self.print = slapo.op.Print()
            self.linear1 = nn.Linear(hidden_size, hidden_size)
            self.activation = nn.GELU()
            self.linear2 = nn.Linear(hidden_size, hidden_size)

        def forward(self, data):
            out = self.linear1(data)
            out = self.print(out, f"print1: {out}")
            out = self.print(out, "print2: %s" % str(out))
            self.print(out, f"print3: {out}")
            out = self.activation(out)
            out = self.linear2(out)
            return out









.. GENERATED FROM PYTHON SOURCE LINES 110-111

Again we create a schedule and trace the module.

.. GENERATED FROM PYTHON SOURCE LINES 111-128

.. code-block:: default


    model = MLPWithWrongPrint(4)
    sch = slapo.create_schedule(model)
    sch.trace()

    # And here is the traced torch.fx graph.
    print(sch.mod.code)

    # We can see that the string to be prined in print1 and print2 are evaluated
    # and fixed after tracing. Therefore, the printed values are always like "Proxy(...)"
    # even if we execute the model:
    model, _ = slapo.build(sch, init_weights=False)
    data = torch.randn((2, 2, 4))
    model(data)

    # Also, print3 disappeared in the graph, because its return value is not consumed
    # by the next operator/module and thus is dead code.




.. rst-class:: sphx-glr-script-out

 .. code-block:: none




    def forward(self, data):
        linear1 = self.linear1(data);  data = None
        print_1 = self.print(linear1, 'print1: Proxy(linear1)');  linear1 = None
        print_2 = self.print(print_1, 'print2: Proxy(print_1)');  print_1 = None
        activation = self.activation(print_2);  print_2 = None
        linear2 = self.linear2(activation);  activation = None
        return linear2
    
    print1: Proxy(linear1)
    print2: Proxy(print_1)

    tensor([[[ 0.4767,  0.0653, -0.2783,  0.3378],
             [ 0.1664, -0.2333, -1.1889,  0.6054]],

            [[ 0.2755, -0.0977, -0.7087,  0.4283],
             [ 0.3443, -0.0327, -0.3935,  0.2707]]], grad_fn=<ViewBackward0>)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.018 seconds)


.. _sphx_glr_download_gallery_debug-print.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: debug-print.py <debug-print.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: debug-print.ipynb <debug-print.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
