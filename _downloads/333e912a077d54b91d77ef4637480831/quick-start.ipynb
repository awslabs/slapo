{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: slapo\n\n# Quick Start\n\nThis guide walks through the key functionality of Slapo.\nWe will use the BERT model in [HuggingFace Hub](https://github.com/huggingface/transformers) as an example and leverage Slapo to optimize its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize PyTorch model with Slapo\nWe first import the Slapo package. Make sure you have already installed the PyTorch package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import slapo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then load a BERT model implemented in PyTorch from HuggingFace Hub.\nThis is the model definition part.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from transformers import BertLMHeadModel, AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"bert-large-uncased\")\nbert = BertLMHeadModel(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we have the model defintion, we can create a default schedule `sch`.\nLater on, all the optimizations will be conducted on this schedule,\nand we do not need to directly modify the original model.\nThe original module is stored in the :class:`~slapo.Schedule`, and can be accessed by `sch.mod`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch = slapo.create_schedule(bert)\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above output, we can see that the original hierachical structure\nis preserved in Slapo schedule. Actually, since we have not added any\noptimizations, the model should be exactly the same as the vanilla PyTorch one.\nUsers can leverage this structure to access the inner modules to\nconduct optimizations. For example, we can use the following code to access\nthe first attention layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subsch = sch[\"bert.encoder.layer.0.attention\"]\nprint(subsch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output submodule is just a part of the original one, but this helps users\nto quickly locate the submodule they need.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subsch = sch[\"bert.encoder.layer.0.intermediate\"]\nprint(subsch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We next optimize the Feed-Forward Network (FFN) part. We try to conduct operator\nfusion for linear bias and GeLU function. As we want to conduct other optimizations\nfor linear weight (e.g., sharding), we cannot fuse it with consequential operators.\nTherefore, we need to decompose the bias from the `nn.Linear` module, \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subsch[\"dense\"].decompose()\nprint(subsch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since operator fusion requires a static dataflow graph, we call the `.trace()` function\nto obtain the graph. As we want to get the inner operators of the linear layer, we also\nneed to specify the `flatten` keyword in order to let the tracer trace into it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subsch.trace(flatten=True)\nprint(subsch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we obtain the dataflow graph, we can define the fusion pattern and leverage\n`.find()` primitive to retrieve the subgraph.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  The returned nodes are reprented in a tuple, where the first element is the\n  path of the node (i.e., its parent module's name), and the second element is\n  the actual `fx.Node`.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\ndef bias_gelu_pattern(x, bias):\n    return F.gelu(x + bias)\n\nsubgraphs = subsch.find(bias_gelu_pattern)\nprint(subgraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output is a list of subgraphs that contains a list of nodes satisfying the\npattern requirement. For this case, there are two operators, named `add` and `gelu`.\nThat is what we want for the pattern. We then pass it into TorchScript compiler \nand fuse the operators.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subsch.fuse(subgraphs, compiler=\"TorchScript\", name=\"FusedBiasGeLU\")\nprint(subsch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the above result that the linear bias and GeLU function are indeed\nfused together and form a new module named `FusedBiasGeLU_0`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}