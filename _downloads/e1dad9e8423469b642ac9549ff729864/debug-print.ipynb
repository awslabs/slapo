{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: slapo\n\n# Debugging with Print\n\nAlthough Slapo only traces a sub-module when we have to schedule its computational\ngraph, it is still annoying to debug the numerical correctness of traced sub-modules.\nOne important reason is that the traced sub-module becomes a GraphModule, which\ncomputational graph is the traced IR graph in torch.fx. It means the forward function\nof the sub-module is only evaluated when generating torch.fx IR graph instead of\nthe model execution. Therefore, we cannot print the intermediate values of\nthe sub-module during runtime.\n\nTo solve this problem, we provide a custom module ``Print`` in Slapo. This module\nis marked as a leaf in our tracer, which means it will be preserved in the traced\ngraph and can be evaluated in runtime.\n\nIn this turorial, we will show how to use ``Print`` to print the intermediate\nvalues of a sub-module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first import the Slapo package. Make sure you have already installed PyTorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport slapo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a MLP module that consists of two linear layers and a GELU activation\nas an example in this tutorial. You can notice that we add a ``Print`` module\nto print the intermediate output of the first Linear layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPWithPrint(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.print = slapo.op.Print()\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.activation = nn.GELU()\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, data):\n        out = self.linear1(data)\n        out = self.print(out, \"linear1 shape\\n\", out.shape, \"\\nvalue\\n\", out)\n        out = self.activation(out)\n        out = self.linear2(out)\n        return out\n\n\n# You may feel the usage of `self.print` looks weird. This is because `self.print`\n# has to return a tensor, and the returned tensor has to be consumed by the\n# next operator/module, making it a part of the dataflow graph; otherwise\n# `self.print` will be removed by the tracer because it is dead code.\n# From a dataflow's point of view, you can treat `out = self.print(out, ...)` as\n# a statement of identical assignment (i.e., `out = out`).\n\n# Starting from the second argument are the arguments of normal Python `print`.\n# However, you have to make sure the values you printed are evaluated lazily.\n# Specifically, in this example, we specify `out` in the 3rd argument instead of\n# a part of the string in 2nd argument, so that it will be evaluated in runtime.\n# We will show some incorrect usages of `self.print` in the end of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's create a schedule and trace the module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = MLPWithPrint(4)\nsch = slapo.create_schedule(model)\nsch.trace()\n\n# And here is the traced torch.fx graph. We can see that `self.print` becomes\n# an operator in the graph with the output of linear1 as its arguments.\nprint(sch.mod.code)\n\n# We then build and execute the model:\nmodel, _ = slapo.build(sch, init_weights=False)\ndata = torch.randn((2, 2, 4))\nmodel(data)\n\n# The linear1's output is printed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the other hand, as we have mentioned above, the print won't work properly\nif the values you want to print are evaluated when tracing. Here is an example\nthat shows incorrect usages of `self.print`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPWithWrongPrint(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.print = slapo.op.Print()\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.activation = nn.GELU()\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, data):\n        out = self.linear1(data)\n        out = self.print(out, f\"print1: {out}\")\n        out = self.print(out, \"print2: %s\" % str(out))\n        self.print(out, f\"print3: {out}\")\n        out = self.activation(out)\n        out = self.linear2(out)\n        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we create a schedule and trace the module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = MLPWithWrongPrint(4)\nsch = slapo.create_schedule(model)\nsch.trace()\n\n# And here is the traced torch.fx graph.\nprint(sch.mod.code)\n\n# We can see that the string to be prined in print1 and print2 are evaluated\n# and fixed after tracing. Therefore, the printed values are always like \"Proxy(...)\"\n# even if we execute the model:\nmodel, _ = slapo.build(sch, init_weights=False)\ndata = torch.randn((2, 2, 4))\nmodel(data)\n\n# Also, print3 disappeared in the graph, because its return value is not consumed\n# by the next operator/module and thus is dead code."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}