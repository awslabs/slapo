{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: slapo\n\n# Optimize Attention Module on A Single GPU\n\nThis guide uses the [Attention](https://arxiv.org/abs/1706.03762) module,\nthe core and most time-consuming module in Transformer-based models, as an\nexample to show how we can leverage Slapo to optimize its performance on\na single GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first import the necessary packages. Make sure you have already installed\nthe PyTorch framework.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport slapo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Attention module is consisted of SelfAttention and Projection modules, where\nSelfAttention takes in the hidden states and pass it through three diffferent\nlinear layers to generate the query, key and value tensors. Then, those tensors\nwill be performed the following scaled dot-product attention:\n\n$$ \\mathrm{CoreAttention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt(d_k)}\\right) * V $$\n\nwhere $d_k$ is the hidden dimension. Finally, the output of the attention module\nwill be passed through a linear projection layer, added with the residual\nconnection, and conducted a layer norm to generate the final output.\nThe following code shows the implementation of the Attention module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v):\n    # (bs, head, seq, hs // head)\n    d_k = q.shape[-1]\n    attn_score = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n    # (bs, head, seq, seq)\n    attn_probs = F.softmax(attn_score, dim=-1)\n    attn_probs = F.dropout(attn_probs, 0.1)\n    # (bs, head, seq, hs // head)\n    attn = torch.matmul(attn_probs, v)\n    return attn\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q_proj = nn.Linear(1024, 1024)\n        self.k_proj = nn.Linear(1024, 1024)\n        self.v_proj = nn.Linear(1024, 1024)\n        self.dropout = nn.Dropout(0.1)\n        self.n_heads = 16\n\n    def permute_for_scores(self, x):\n        # x: (batch_size, seq_len, hidden_size)\n        new_shape = x.shape[:-1] + (self.n_heads, -1)\n        x = x.view(new_shape)\n        # output: (bs, head, seq, hs // head)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        # hidden_states: (batch_size, seq_len, hidden_size)\n        # qkv layers\n        q = self.permute_for_scores(self.q_proj(hidden_states))\n        k = self.permute_for_scores(self.k_proj(hidden_states))\n        v = self.permute_for_scores(self.v_proj(hidden_states))\n        # core attention\n        output = scaled_dot_product(q, k, v)\n        # output: (bs, seq, head, hs // head)\n        output.permute(0, 2, 1, 3)\n        output.view(output.shape[0], output.shape[1], -1)\n        return output\n\n\nclass Projection(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dense = nn.Linear(1024, 1024)\n        self.dropout = nn.Dropout(0.1)\n        self.layer_norm = nn.LayerNorm(1024)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.layer_norm(hidden_states, input_tensor)\n        return hidden_states\n\n\nclass Attention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.self_attn = SelfAttention()\n        self.proj = Projection()\n\n    def forward(self, hidden_states):\n        self_output = self.self_attn(hidden_states)\n        attention_output = self.proj(self_output, hidden_states)\n        return attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Users can instantiate the model based on the above definition as usual.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Later, we pass the model to Slapo and create a default schedule for it.\nThe schedule always includes the original or the transformed module.\nUsers can check the module by calling the ``mod`` attribute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch = slapo.create_schedule(model)\nprint(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, Slapo works seamlessly with the PyTorch models and preserves\nthe hierachical structure of the original model. As we have not added any\noptimizations, the module is exactly the same as the original one.\nThis is also the idea of progressive optimization -- we only apply optimizations\nto a small part of the model at a time and do not affect other parts.\nIf no optimizations are applied, then no changes will be made to the model, which\nis different from the traditional static graph optimization employed by deep\nlearning compilers.\n\nIn the following, we will show how to gradually apply optimizations to the model.\nSince the three linear layers in the SelfAttention module are independent, we\ncan merge them into a single linear layer to reduce the number of GEMM\noperations, and thus reduce the GPU kernel launch overheads.\n\nThe first thing to do is to find those three linear layers and the consequential\noperations in the model. Slapo provides an easy-to-use API to help users\ndefine the pattern and find the corresponding module or subgraph in the model.\nWe can define a subgraph pattern function as shown below. The ``call_module``\nfunction will try to match a call node that satisfies the user-defined\nconstraint in the dataflow graph. The first argument specifies the name of the\nmodule, where regular expression is supported, so the it can support fuzzy\nmatching in this case. The latter arguments are the arguments of the call node.\nUsing this function, we can use just one line of code to match the three linear\nlayers. Also, we need to incorporate the ``view`` and ``permute`` operations,\nwhich should also be fused together instead of doing three times separately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from slapo.pattern import call_module\n\ndef pattern(x):\n    x = call_module(r\"[qkv]_proj\", x)\n    new_shape = x.shape[:-1] + (16, -1)\n    x = x.view(new_shape)\n    return x.permute(0, 2, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the pattern, we can use the ``.find()`` primitive to find the\ncorresponding subgraph in the model. It will return a list of subgraphs that\nmatch the pattern. In our case, there will be three subgraphs, one for each\nlinear layer and the ``view`` and ``permute`` operations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subgraphs = sch.find(pattern)\nprint(subgraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can later define a fused QKV layer to replace the three linear layers.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}