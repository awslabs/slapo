

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Optimize Attention Module on A Single Device &#8212; Slapo Documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gallery/attention-single-gpu';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimize MLP Module on Multi-Device" href="mlp-multi-gpu.html" />
    <link rel="prev" title="Quick Start" href="quick-start.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">Slapo Documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../setup/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick-start.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Optimize Attention Module on A Single Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp-multi-gpu.html">Optimize MLP Module on Multi-Device</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../python_api/index.html">Python API</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_api/root.html">slapo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/schedule.html">slapo.schedule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/initialization.html">slapo.initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/pattern.html">slapo.pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/pipeline.html">slapo.pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/tracer.html">slapo.tracer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_api/random.html">slapo.random</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../python_api/framework_dialect/index.html">slapo.framework_dialect</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../python_api/framework_dialect/registry.html">slapo.framework_dialect.registry</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../python_api/framework_dialect/deepspeed/index.html">slapo.framework_dialect.deepspeed</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../python_api/framework_dialect/deepspeed/engine.html">slapo.framework_dialect.deepspeed.engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../python_api/framework_dialect/deepspeed/pipeline.html">slapo.framework_dialect.deepspeed.pipeline</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../python_api/model_schedule/index.html">slapo.model_schedule</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../python_api/model_schedule/api.html">slapo.model_schedule.api</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../python_api/op/index.html">slapo.op</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/attention.html">slapo.op.attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/cross_entropy.html">slapo.op.cross_entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/fused_bias.html">slapo.op.fused_bias</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/linear.html">slapo.op.linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="../python_api/op/mlp.html">slapo.op.mlp</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/awslabs/slapo" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/gallery/attention-single-gpu.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimize Attention Module on A Single Device</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">Model Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model-schedule">Create Model Schedule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-selfattention-module">Optimize SelfAttention Module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replace-qkv-linear-layers">Replace QKV Linear Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replace-scaled-dot-product-attention">Replace Scaled Dot-Product Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-the-projection-module">Optimize the Projection Module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-the-optimized-model">Build the Optimized Model</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-gallery-attention-single-gpu-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="optimize-attention-module-on-a-single-device">
<span id="sphx-glr-gallery-attention-single-gpu-py"></span><h1>Optimize Attention Module on A Single Device<a class="headerlink" href="#optimize-attention-module-on-a-single-device" title="Permalink to this headline"><span>¶</span></a></h1>
<p>This guide uses the <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention</a> module,
the core and most time-consuming module in Transformer-based models, as an
example to show how we can leverage Slapo to optimize its performance on
a single device. We will cover module tracing, pattern matching, operator
fusion, and partial module replacement in this tutorial.</p>
<p>We first import the necessary packages. Make sure you have already installed
the PyTorch framework.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">slapo</span>
</pre></div>
</div>
<div class="section" id="model-definition">
<h2>Model Definition<a class="headerlink" href="#model-definition" title="Permalink to this headline"><span>¶</span></a></h2>
<p>The Attention module consists of SelfAttention and Projection modules, where
SelfAttention takes in the hidden states and passes it through three different
linear layers to generate the query, key and value tensors. Then, those tensors
will be performed the following scaled dot-product attention:</p>
<div class="math notranslate nohighlight">
\[\mathrm{CoreAttention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^{\mathrm{T}}}{\sqrt{d_k}}\right) \cdot V\]</div>
<p>where <span class="math notranslate nohighlight">\(d_k\)</span> is the hidden dimension. Finally, the output of the attention
module will be passed through a linear projection layer, added with the residual
connection, and conducted a layer norm to generate the final output.
The following code shows the implementation of the Attention module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scaled_dot_product</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># (bs, head, seq, hs // head)</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">attn_score</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul" title="torch.matmul" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span></a><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <a href="https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span></a><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    <span class="c1"># (bs, head, seq, seq)</span>
    <span class="n">attn_probs</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">softmax</span></a><span class="p">(</span><span class="n">attn_score</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attn_probs</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout" title="torch.nn.functional.dropout" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">dropout</span></a><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="c1"># (bs, head, seq, hs // head)</span>
    <span class="n">attn</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul" title="torch.matmul" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span></a><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attn</span>


<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>

    <span class="k">def</span> <span class="nf">permute_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch_size, seq_len, hidden_size)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="c1"># output: (bs, head, seq, hs // head)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="c1"># hidden_states: (batch_size, seq_len, hidden_size)</span>
        <span class="c1"># qkv layers</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">permute_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">permute_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">permute_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
        <span class="c1"># core attention</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">scaled_dot_product</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="c1"># output: (bs, seq, head, hs // head)</span>
        <span class="n">output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="k">class</span> <span class="nc">Projection</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout" title="torch.nn.Dropout" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span></a><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">input_tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">SelfAttention</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">Projection</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">self_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">self_output</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</pre></div>
</div>
<p>Users can instantiate the model based on the above definition as usual.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">Attention</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="create-model-schedule">
<h2>Create Model Schedule<a class="headerlink" href="#create-model-schedule" title="Permalink to this headline"><span>¶</span></a></h2>
<p>Later, we pass the model to Slapo and create a default schedule for it.
The schedule always includes the original or the transformed module.
Users can check the module by calling the <code class="docutils literal notranslate"><span class="pre">mod</span></code> attribute.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sch</span> <span class="o">=</span> <span class="n">slapo</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Attention(
  (self_attn): SelfAttention(
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (proj): Projection(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
)
</pre></div>
</div>
<p>As we can see, Slapo works seamlessly with the PyTorch models and preserves
the hierarchical structure of the original model. As we have not added any
optimizations, the module is exactly the same as the original one.
We can easily obtain the submodules by passing the module name to the schedule,
which will return a new schedule for the submodule.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">attn_sch</span> <span class="o">=</span> <span class="n">sch</span><span class="p">[</span><span class="s2">&quot;self_attn&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SelfAttention(
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
</pre></div>
</div>
<p>This is also the idea of progressive optimization – we only apply optimizations
to a small part of the model at a time and do not affect other parts.
If no optimizations are applied, then no changes will be made to the model, which
is different from the traditional static graph optimization employed by deep
learning compilers.</p>
<p>In the following, we will show how to gradually apply optimizations to the model.</p>
</div>
<div class="section" id="optimize-selfattention-module">
<h2>Optimize SelfAttention Module<a class="headerlink" href="#optimize-selfattention-module" title="Permalink to this headline"><span>¶</span></a></h2>
<div class="section" id="replace-qkv-linear-layers">
<h3>Replace QKV Linear Layers<a class="headerlink" href="#replace-qkv-linear-layers" title="Permalink to this headline"><span>¶</span></a></h3>
<p>Since the three linear layers in the SelfAttention module are independent, we
can merge them into a single linear layer to reduce the number of GEMM
operations, and thus reduce the kernel launch overheads.</p>
<p>The first thing to do is to find those three linear layers and the consequential
operations in the model. Slapo provides an easy-to-use API to help users
define the pattern and find the corresponding module or subgraph in the model.
We can define a subgraph pattern function as shown below. The <code class="docutils literal notranslate"><span class="pre">call_module</span></code>
function will try to match a call node that satisfies the user-defined
constraint in the dataflow graph. The first argument specifies the name of the
module, where regular expression is supported, so it can support fuzzy
matching in this case. The latter arguments are the arguments of the call node.
Using this function, we can use just one line of code to match the three linear
layers. Also, we need to incorporate the <code class="docutils literal notranslate"><span class="pre">view</span></code> and <code class="docutils literal notranslate"><span class="pre">permute</span></code> operations,
which should also be fused together instead of doing three times separately.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">slapo.pattern</span> <span class="kn">import</span> <span class="n">call_module</span>


<span class="k">def</span> <span class="nf">pattern</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">call_module</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[qkv]_proj&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>After defining the pattern, we can use the <code class="docutils literal notranslate"><span class="pre">.find()</span></code> primitive to find the
corresponding subgraph in the model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">qkv_subgraphs</span> <span class="o">=</span> <span class="n">attn_sch</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span>
</pre></div>
</div>
<p>The primitive basically does two things. First, it will <cite>implicitly</cite> trace the submodule
into a static subgraph. Currently, we use <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">torch.fx</a>
as the IR, so the traced module will become a <code class="docutils literal notranslate"><span class="pre">torch.fx.GraphModule</span></code>, and we can also
see the forward function of it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">attn_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>GraphModule(
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
)



def forward(self, hidden_states):
    q_proj = self.q_proj(hidden_states)
    getattr_1 = q_proj.shape
    getitem = getattr_1[slice(None, -1, None)];  getattr_1 = None
    add = getitem + (16, -1);  getitem = None
    view = q_proj.view(add);  q_proj = add = None
    permute = view.permute(0, 2, 1, 3);  view = None
    k_proj = self.k_proj(hidden_states)
    getattr_2 = k_proj.shape
    getitem_1 = getattr_2[slice(None, -1, None)];  getattr_2 = None
    add_1 = getitem_1 + (16, -1);  getitem_1 = None
    view_1 = k_proj.view(add_1);  k_proj = add_1 = None
    permute_1 = view_1.permute(0, 2, 1, 3);  view_1 = None
    v_proj = self.v_proj(hidden_states);  hidden_states = None
    getattr_3 = v_proj.shape
    getitem_2 = getattr_3[slice(None, -1, None)];  getattr_3 = None
    add_2 = getitem_2 + (16, -1);  getitem_2 = None
    view_2 = v_proj.view(add_2);  v_proj = add_2 = None
    permute_2 = view_2.permute(0, 2, 1, 3);  view_2 = None
    getattr_4 = permute.shape
    getitem_3 = getattr_4[-1];  getattr_4 = None
    transpose = permute_1.transpose(-2, -1);  permute_1 = None
    matmul = torch.matmul(permute, transpose);  permute = transpose = None
    sqrt = torch.sqrt(getitem_3);  getitem_3 = None
    truediv = matmul / sqrt;  matmul = sqrt = None
    softmax = torch.nn.functional.softmax(truediv, dim = -1, _stacklevel = 3, dtype = None);  truediv = None
    dropout = torch.nn.functional.dropout(softmax, p = 0.1, training = True, inplace = False);  softmax = None
    matmul_1 = torch.matmul(dropout, permute_2);  dropout = permute_2 = None
    return matmul_1

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>Second, the <code class="docutils literal notranslate"><span class="pre">.find()</span></code> primitive will return a list of subgraphs that
match the pattern. In our case, there will be three subgraphs, one for each
linear layer and the consequential <code class="docutils literal notranslate"><span class="pre">view</span></code> and <code class="docutils literal notranslate"><span class="pre">permute</span></code> operations.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">qkv_subgraphs</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[(&#39;&#39;, q_proj), (&#39;&#39;, getattr_1), (&#39;&#39;, getitem), (&#39;&#39;, add), (&#39;&#39;, view), (&#39;&#39;, permute)], [(&#39;&#39;, k_proj), (&#39;&#39;, getattr_2), (&#39;&#39;, getitem_1), (&#39;&#39;, add_1), (&#39;&#39;, view_1), (&#39;&#39;, permute_1)], [(&#39;&#39;, v_proj), (&#39;&#39;, getattr_3), (&#39;&#39;, getitem_2), (&#39;&#39;, add_2), (&#39;&#39;, view_2), (&#39;&#39;, permute_2)]]
</pre></div>
</div>
<p>Then, we define a fused QKV module as follows and instantiate it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FusedQKV</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fused_linear</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">permute_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_linear</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">reshaped_qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">permute_for_scores</span><span class="p">(</span><span class="n">qkv</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.split.html#torch.split" title="torch.split" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">split</span></a><span class="p">(</span><span class="n">reshaped_qkv</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze" title="torch.squeeze" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze" title="torch.squeeze" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze" title="torch.squeeze" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span>


<span class="n">fused_qkv</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">FusedQKV</span></a><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
<p>We can replace the subgraphs with the fused QKV module by calling the
<code class="docutils literal notranslate"><span class="pre">.replace()</span></code> primitive. The first argument is the new module,
and the second argument is the subgraph to be replaced.
After replacing the subgraph, we can check the model again to see the
changes.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">attn_sch</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">fused_qkv</span><span class="p">,</span> <span class="n">qkv_subgraphs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>GraphModule(
  (FusedQKV_0): FusedQKV(
    (fused_linear): Linear(in_features=1024, out_features=3072, bias=True)
  )
)



def forward(self, hidden_states):
    fused_qkv_0 = self.FusedQKV_0(hidden_states);  hidden_states = None
    getitem_8 = fused_qkv_0[2]
    getitem_7 = fused_qkv_0[1]
    getitem_6 = fused_qkv_0[0];  fused_qkv_0 = None
    getattr_4 = getitem_6.shape
    getitem_3 = getattr_4[-1];  getattr_4 = None
    transpose = getitem_7.transpose(-2, -1);  getitem_7 = None
    matmul = torch.matmul(getitem_6, transpose);  getitem_6 = transpose = None
    sqrt = torch.sqrt(getitem_3);  getitem_3 = None
    truediv = matmul / sqrt;  matmul = sqrt = None
    softmax = torch.nn.functional.softmax(truediv, dim = -1, _stacklevel = 3, dtype = None);  truediv = None
    dropout = torch.nn.functional.dropout(softmax, p = 0.1, training = True, inplace = False);  softmax = None
    matmul_1 = torch.matmul(dropout, getitem_8);  dropout = getitem_8 = None
    return matmul_1

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>From the above output, we can see there is a new module called <code class="docutils literal notranslate"><span class="pre">FusedQKV_0</span></code>
with <span class="math notranslate nohighlight">\(3\times\)</span> <code class="docutils literal notranslate"><span class="pre">out_features</span></code> compared to the original linear layer.
The corresponding forward function is also changed to leverage the fused
module.</p>
</div>
<div class="section" id="replace-scaled-dot-product-attention">
<h3>Replace Scaled Dot-Product Attention<a class="headerlink" href="#replace-scaled-dot-product-attention" title="Permalink to this headline"><span>¶</span></a></h3>
<p>Next, we still use the <code class="docutils literal notranslate"><span class="pre">.find()</span></code> primitive to find the core attention function
and replace it with a more efficient implementation. Different from the QKV example
that requires us to explicitly write the fuzzy pattern, we can directly write
a function with the identical computation subgraph as the pattern. Since the
<code class="docutils literal notranslate"><span class="pre">scaled_dot_product</span></code> function has been defined previously, we can reuse it
and pass it into <code class="docutils literal notranslate"><span class="pre">.find()</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">core_attn_subgraph</span> <span class="o">=</span> <span class="n">attn_sch</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">scaled_dot_product</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">core_attn_subgraph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[(&#39;&#39;, getattr_4), (&#39;&#39;, getitem_3), (&#39;&#39;, transpose), (&#39;&#39;, matmul), (&#39;&#39;, sqrt), (&#39;&#39;, truediv), (&#39;&#39;, softmax), (&#39;&#39;, dropout), (&#39;&#39;, matmul_1)]]
</pre></div>
</div>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">FlashAttentionOp</span></code> provided by Slapo that makes use
of <a class="reference external" href="https://arxiv.org/abs/2205.14135">flash attention</a> kernels from
<a class="reference external" href="https://github.com/facebookresearch/xformers">xFormers</a> and
<a class="reference external" href="https://github.com/HazyResearch/flash-attention">flash-attention</a> libraries
to replace the core attention. We directly import and replace the subgraph
with <code class="docutils literal notranslate"><span class="pre">FlashAttentionOp</span></code>.
Notice, since the <code class="docutils literal notranslate"><span class="pre">scaled_dot_product</span></code> function we defined above only accepts
the <code class="docutils literal notranslate"><span class="pre">query</span></code>, <code class="docutils literal notranslate"><span class="pre">key</span></code>, and <code class="docutils literal notranslate"><span class="pre">value</span></code> tensors, while <code class="docutils literal notranslate"><span class="pre">FlashAttentionOp</span></code> requires
five arguments, so we need to explicitly pass <code class="docutils literal notranslate"><span class="pre">None</span></code> to the <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>
argument, and set the dropout probability <code class="docutils literal notranslate"><span class="pre">p</span></code> to 0.1 by setting the <code class="docutils literal notranslate"><span class="pre">concrete_args</span></code>.</p>
<div class="margin admonition note">
<p class="admonition-title">Note</p>
<p>We use <code class="docutils literal notranslate"><span class="pre">native_xformers</span></code> in this tutorial to demonstrate the functionality.
In reality, users can choose <code class="docutils literal notranslate"><span class="pre">cutlass</span></code>, <code class="docutils literal notranslate"><span class="pre">triton</span></code>, or <code class="docutils literal notranslate"><span class="pre">cuda</span></code> kernels to achieve
better performance, while the latter two only support NVIDIA V100 GPU.
Please refer to <cite>slapo.op.attention.FlashAttentionOp</cite> for more details.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">slapo.op.attention</span> <span class="kn">import</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">FlashAttentionOp</span></a>

<span class="n">flash_attn</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">FlashAttentionOp</span></a><span class="p">(</span><span class="n">attn_op_name</span><span class="o">=</span><span class="s2">&quot;native_xformers&quot;</span><span class="p">,</span> <span class="n">apply_causal_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">attn_sch</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
    <span class="n">flash_attn</span><span class="p">,</span> <span class="n">core_attn_subgraph</span><span class="p">,</span> <span class="n">concrete_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;p&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>GraphModule(
  (FusedQKV_0): FusedQKV(
    (fused_linear): Linear(in_features=1024, out_features=3072, bias=True)
  )
  (FlashAttentionOp_0): FlashAttentionOp()
)



def forward(self, hidden_states):
    fused_qkv_0 = self.FusedQKV_0(hidden_states);  hidden_states = None
    getitem_8 = fused_qkv_0[2]
    getitem_7 = fused_qkv_0[1]
    getitem_6 = fused_qkv_0[0];  fused_qkv_0 = None
    flash_attention_op_0 = self.FlashAttentionOp_0(getitem_6, getitem_7, getitem_8, attention_mask = None, p = 0.1);  getitem_6 = getitem_7 = getitem_8 = None
    return flash_attention_op_0

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>Again, the <code class="docutils literal notranslate"><span class="pre">FlashAttentionOp</span></code> is attached to the GraphModule, and the forward
function becomes much simpler to call those two submodules.</p>
</div>
</div>
<div class="section" id="optimize-the-projection-module">
<h2>Optimize the Projection Module<a class="headerlink" href="#optimize-the-projection-module" title="Permalink to this headline"><span>¶</span></a></h2>
<p>We then optimize the <code class="docutils literal notranslate"><span class="pre">Projection</span></code> module. A common practice is to fuse the
dropout and the layer norm layer with those element-wise addition operations.
We first create a subschedule for the <code class="docutils literal notranslate"><span class="pre">Projection</span></code> module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">proj_sch</span> <span class="o">=</span> <span class="n">sch</span><span class="p">[</span><span class="s2">&quot;proj&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">proj_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Projection(
  (dense): Linear(in_features=1024, out_features=1024, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
</pre></div>
</div>
<p>As we want to fuse the linear bias with the consequential layers, we need to
decompose the linear layer into two separate matrix multiplication and bias add operations.
In Slapo, this is easy to achieve by simply calling <code class="docutils literal notranslate"><span class="pre">.decompose()</span></code> on the linear module.</p>
<div class="margin admonition note">
<p class="admonition-title">Note</p>
<p>The default <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">nn.Linear</a> module
in PyTorch will directly pass both weight and bias to the backend
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html">F.linear</a> function,
and dispatch it to the corresponding C/CUDA library, so there is no way to fuse the bias if we
do not take it apart. Another reason for decomposing is that we can still optimize the
<code class="docutils literal notranslate"><span class="pre">weight</span></code> parameter later (e.g., sharding) even though the <code class="docutils literal notranslate"><span class="pre">bias</span></code> may be fused.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">proj_sch</span><span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">decompose</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">proj_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Projection(
  (dense): LinearWithSeparateBias(in_features=1024, out_features=1024, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
</pre></div>
</div>
<p>We can see the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> module changed into <code class="docutils literal notranslate"><span class="pre">LinearWithSeparateBias</span></code>, and other submodules
remain the same. Next, we need to <cite>explicitly</cite> call the <code class="docutils literal notranslate"><span class="pre">.trace()</span></code> primitive to trace the module
into a static subgraph. It gives us more control over the traced module. For example, we can
pass in the <code class="docutils literal notranslate"><span class="pre">flatten</span></code> flag to let the tracer gets into each submodule so that the bias add
can be depicted as a node in the subgraph.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">proj_sch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">proj_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>GraphModule(
  (dense): Module()
  (dropout): Dropout(p=0.1, inplace=False)
  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)



def forward(self, hidden_states, input_tensor):
    dense_weight = self.dense.weight
    linear = torch._C._nn.linear(hidden_states, dense_weight, None);  hidden_states = dense_weight = None
    dense_bias = self.dense.bias
    add = linear + dense_bias;  linear = dense_bias = None
    dropout = self.dropout(add);  add = None
    add_1 = dropout + input_tensor;  dropout = input_tensor = None
    layer_norm = self.layer_norm(add_1);  add_1 = None
    return layer_norm

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>We can again define the fusion pattern as follows. Here the pattern includes three input arguments,
Slapo can still handle it correctly and grab all the required nodes in the subgraph.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_pattern</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">residual</span><span class="p">):</span>
    <span class="k">return</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm" title="torch.nn.functional.layer_norm" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">layer_norm</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout" title="torch.nn.functional.dropout" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">dropout</span></a><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>


<span class="n">ln_subgraph</span> <span class="o">=</span> <span class="n">proj_sch</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">ln_pattern</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ln_subgraph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[(&#39;&#39;, add), (&#39;&#39;, dropout), (&#39;&#39;, add_1), (&#39;&#39;, layer_norm)]]
</pre></div>
</div>
<p>For this case of vertical fusion, Slapo provides a <code class="docutils literal notranslate"><span class="pre">.fuse()</span></code> primitive to easily fuse the subgraph.
Users can specify the backend fusion compiler and the name of the fused module. By default, Slapo
will use TorchScript with nvFuser to fuse the subgraph.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">proj_sch</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">ln_subgraph</span><span class="p">,</span> <span class="n">compiler</span><span class="o">=</span><span class="s2">&quot;TorchScript&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;FusedLayerNorm&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">proj_sch</span><span class="o">.</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn&#39;t support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(&quot;The TorchScript type system doesn&#39;t support &quot;
GraphModule(
  (dense): Module()
  (FusedLayerNorm_0): RecursiveScriptModule(
    original_name=GraphModule
    (dropout): RecursiveScriptModule(original_name=Dropout)
    (layer_norm): RecursiveScriptModule(original_name=LayerNorm)
  )
)



def forward(self, hidden_states, input_tensor):
    dense_weight = self.dense.weight
    linear = torch._C._nn.linear(hidden_states, dense_weight, None);  hidden_states = dense_weight = None
    dense_bias = self.dense.bias
    fused_layer_norm_0 = self.FusedLayerNorm_0(linear, dense_bias, input_tensor);  linear = dense_bias = input_tensor = None
    return fused_layer_norm_0

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>As shown in the above output, the <code class="docutils literal notranslate"><span class="pre">FusedLayerNorm</span></code> module is attached to the GraphModule, and
only <code class="docutils literal notranslate"><span class="pre">torch._C._nn.Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">FusedLayerNorm</span></code> are called in the forward function.</p>
</div>
<div class="section" id="build-the-optimized-model">
<h2>Build the Optimized Model<a class="headerlink" href="#build-the-optimized-model" title="Permalink to this headline"><span>¶</span></a></h2>
<p>Finally, we finish all the optimizations for Attention module on a single device.
We can pass the schedule into <code class="docutils literal notranslate"><span class="pre">sch.build</span></code> to build the optimized model for execution.
It returns the optimized model and a default optimizer. We can print out the top-level module
to see the changes. The optimizations are clearly reflected in the new module, and we still
keep the module hierarchy, which greatly enhances the readability and debuggability of the code.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">opt_model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">slapo</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">init_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">opt_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Attention(
  (self_attn): GraphModule(
    (FusedQKV_0): FusedQKV(
      (fused_linear): Linear(in_features=1024, out_features=3072, bias=True)
    )
    (FlashAttentionOp_0): FlashAttentionOp()
  )
  (proj): GraphModule(
    (dense): Module()
    (FusedLayerNorm_0): RecursiveScriptModule(
      original_name=GraphModule
      (dropout): RecursiveScriptModule(original_name=Dropout)
      (layer_norm): RecursiveScriptModule(original_name=LayerNorm)
    )
  )
)
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.121 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-gallery-attention-single-gpu-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4011ed9eb8a93a419ccf3359a7195911/attention-single-gpu.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">attention-single-gpu.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/75035efa7a65b528e8e8d888994df769/attention-single-gpu.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">attention-single-gpu.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="quick-start.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quick Start</p>
      </div>
    </a>
    <a class="right-next"
       href="mlp-multi-gpu.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Optimize MLP Module on Multi-Device</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-definition">Model Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-model-schedule">Create Model Schedule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-selfattention-module">Optimize SelfAttention Module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replace-qkv-linear-layers">Replace QKV Linear Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replace-scaled-dot-product-attention">Replace Scaled Dot-Product Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-the-projection-module">Optimize the Projection Module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-the-optimized-model">Build the Optimized Model</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Amazon
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, Amazon.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>